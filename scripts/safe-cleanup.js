#!/usr/bin/env node
/**
 * SAFE DATABASE CLEANUP - Step by Step
 * With live verification and rollback capability
 */

const { createClient } = require('@supabase/supabase-js');
const fs = require('fs').promises;
const path = require('path');

const SUPABASE_URL = 'https://trkgvxizmmhtvdfmgxnn.supabase.co';
const SUPABASE_KEY = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InRya2d2eGl6bW1odHZkZm1neG5uIiwicm9sZSI6ImFub24iLCJpYXQiOjE2NzY0NTc5MTcsImV4cCI6MTk5MjAzMzkxN30.xRZydpUO3fCZh-jfkZOdPcLy8458eZMyenCxGCwXg8g';

const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);

const STEPS = [
  {
    name: 'Export Critical Data',
    description: 'Save all data before any changes',
    action: exportCriticalData
  },
  {
    name: 'Create Archive Tables',
    description: 'Create tables to store archived data',
    action: createArchiveTables
  },
  {
    name: 'Archive Old Cashback Codes',
    description: 'Move 1000 redeemed codes from 2023 to archive',
    action: archiveOldCodes
  },
  {
    name: 'Archive Old Transactions',
    description: 'Move 92 pre-2024 transactions to archive',
    action: archiveOldTransactions
  },
  {
    name: 'Remove Duplicate SMS',
    description: 'Delete 31 duplicate SMS messages',
    action: removeDuplicateSMS
  },
  {
    name: 'Final Verification',
    description: 'Verify database size and data integrity',
    action: finalVerification
  }
];

async function exportCriticalData() {
  console.log('\nüì¶ STEP 1: Exporting critical data before cleanup...\n');
  
  const exportDir = path.join(__dirname, '../../backups/pre-cleanup-export-' + new Date().toISOString().split('T')[0]);
  await fs.mkdir(exportDir, { recursive: true });
  
  const tables = ['cashback_codes', 'tiler_transaction', 'sms'];
  
  for (const table of tables) {
    console.log(`  Exporting ${table}...`);
    
    let allData = [];
    let offset = 0;
    let hasMore = true;
    
    while (hasMore) {
      const { data, error } = await supabase
        .from(table)
        .select('*')
        .range(offset, offset + 999);
      
      if (error) throw error;
      
      if (data && data.length > 0) {
        allData = allData.concat(data);
        offset += 1000;
        if (data.length < 1000) hasMore = false;
      } else {
        hasMore = false;
      }
    }
    
    await fs.writeFile(
      path.join(exportDir, `${table}.json`),
      JSON.stringify(allData, null, 2)
    );
    
    console.log(`    ‚úÖ Exported ${allData.length} records`);
  }
  
  console.log(`\n  üíæ Data exported to: ${exportDir}`);
  console.log('  üîí This is your safety net - data can be restored from here\n');
  
  return { exportDir };
}

async function createArchiveTables() {
  console.log('\nüì¶ STEP 2: Creating archive tables...\n');
  
  // Check if tables already exist
  const { data: existingTables, error: checkError } = await supabase
    .from('information_schema.tables')
    .select('table_name')
    .eq('table_schema', 'public')
    .in('table_name', ['cashback_codes_archive', 'tiler_transaction_archive', 'sms_archive']);
  
  if (checkError) {
    console.log('  ‚ö†Ô∏è  Could not check existing tables, will attempt to create anyway');
  } else if (existingTables && existingTables.length > 0) {
    console.log('  ‚ÑπÔ∏è  Archive tables already exist:');
    existingTables.forEach(t => console.log(`    - ${t.table_name}`));
    console.log('  ‚úÖ Skipping creation\n');
    return { created: false, existing: existingTables.map(t => t.table_name) };
  }
  
  // Create tables using RPC or direct SQL via REST
  console.log('  Creating cashback_codes_archive...');
  const { error: err1 } = await supabase.rpc('create_archive_tables');
  
  if (err1) {
    console.log('  ‚ö†Ô∏è  RPC not available, tables may need manual creation in SQL Editor');
    console.log('  ‚ÑπÔ∏è  You can proceed - data will be preserved in exported JSON files\n');
    return { created: false, note: 'Manual creation needed' };
  }
  
  console.log('  ‚úÖ Archive tables created\n');
  return { created: true };
}

async function archiveOldCodes() {
  console.log('\nüì¶ STEP 3: Archiving old redeemed cashback codes...\n');
  
  // First, get the codes to archive
  console.log('  Fetching codes to archive (redeemed before 2024)...');
  const { data: codesToArchive, error: fetchError } = await supabase
    .from('cashback_codes')
    .select('*')
    .eq('redeemed', true)
    .lt('_createdAt', '2024-01-01');
  
  if (fetchError) {
    console.log(`  ‚ùå Error: ${fetchError.message}`);
    return { error: fetchError.message };
  }
  
  if (!codesToArchive || codesToArchive.length === 0) {
    console.log('  ‚ÑπÔ∏è  No codes to archive\n');
    return { archived: 0 };
  }
  
  console.log(`  Found ${codesToArchive.length} codes to archive`);
  
  // Try to insert into archive table
  console.log('  Moving to archive table...');
  const { error: insertError } = await supabase
    .from('cashback_codes_archive')
    .upsert(codesToArchive, { onConflict: '_id' });
  
  if (insertError) {
    console.log(`  ‚ö†Ô∏è  Archive insert failed: ${insertError.message}`);
    console.log('  ‚ÑπÔ∏è  Data is preserved in exported JSON file');
    console.log('  ‚ö†Ô∏è  Skipping deletion - manual SQL execution recommended\n');
    return { error: insertError.message, preserved: true };
  }
  
  console.log(`  ‚úÖ Archived ${codesToArchive.length} codes`);
  
  // Now delete from main table
  console.log('  Deleting from main table...');
  const idsToDelete = codesToArchive.map(c => c._id);
  
  // Delete in batches of 100
  let deletedCount = 0;
  for (let i = 0; i < idsToDelete.length; i += 100) {
    const batch = idsToDelete.slice(i, i + 100);
    const { error: deleteError } = await supabase
      .from('cashback_codes')
      .delete()
      .in('_id', batch);
    
    if (deleteError) {
      console.log(`  ‚ö†Ô∏è  Error deleting batch ${i/100 + 1}: ${deleteError.message}`);
    } else {
      deletedCount += batch.length;
      process.stdout.write(`    Progress: ${deletedCount}/${codesToArchive.length}\r`);
    }
  }
  
  console.log(`\n  ‚úÖ Deleted ${deletedCount} codes from main table\n`);
  
  // Verify
  const { count: remainingCount } = await supabase
    .from('cashback_codes')
    .select('*', { count: 'exact', head: true })
    .eq('redeemed', true)
    .lt('_createdAt', '2024-01-01');
  
  console.log(`  üìä Verification: ${remainingCount || 0} old redeemed codes remain (should be 0)\n`);
  
  return { archived: codesToArchive.length, deleted: deletedCount };
}

async function archiveOldTransactions() {
  console.log('\nüì¶ STEP 4: Archiving old tiler transactions...\n');
  
  console.log('  Fetching transactions to archive (pre-2024)...');
  const { data: transToArchive, error: fetchError } = await supabase
    .from('tiler_transaction')
    .select('*')
    .lt('created_at', '2024-01-01');
  
  if (fetchError) {
    console.log(`  ‚ùå Error: ${fetchError.message}`);
    return { error: fetchError.message };
  }
  
  if (!transToArchive || transToArchive.length === 0) {
    console.log('  ‚ÑπÔ∏è  No transactions to archive\n');
    return { archived: 0 };
  }
  
  console.log(`  Found ${transToArchive.length} transactions to archive`);
  
  // Insert to archive
  console.log('  Moving to archive table...');
  const { error: insertError } = await supabase
    .from('tiler_transaction_archive')
    .upsert(transToArchive, { onConflict: 'id' });
  
  if (insertError) {
    console.log(`  ‚ö†Ô∏è  Archive insert failed: ${insertError.message}`);
    console.log('  ‚ÑπÔ∏è  Data preserved in exported JSON');
    return { error: insertError.message, preserved: true };
  }
  
  console.log(`  ‚úÖ Archived ${transToArchive.length} transactions`);
  
  // Delete from main
  console.log('  Deleting from main table...');
  const idsToDelete = transToArchive.map(t => t.id);
  
  let deletedCount = 0;
  for (let i = 0; i < idsToDelete.length; i += 50) {
    const batch = idsToDelete.slice(i, i + 50);
    const { error: deleteError } = await supabase
      .from('tiler_transaction')
      .delete()
      .in('id', batch);
    
    if (deleteError) {
      console.log(`  ‚ö†Ô∏è  Error deleting batch: ${deleteError.message}`);
    } else {
      deletedCount += batch.length;
    }
  }
  
  console.log(`  ‚úÖ Deleted ${deletedCount} transactions from main table\n`);
  
  return { archived: transToArchive.length, deleted: deletedCount };
}

async function removeDuplicateSMS() {
  console.log('\nüì¶ STEP 5: Removing duplicate SMS messages...\n');
  
  console.log('  Analyzing SMS for duplicates...');
  const { data: allSMS, error: fetchError } = await supabase
    .from('sms')
    .select('*');
  
  if (fetchError) {
    console.log(`  ‚ùå Error: ${fetchError.message}`);
    return { error: fetchError.message };
  }
  
  // Group by phone_number + text
  const groups = {};
  allSMS.forEach(sms => {
    const key = `${sms.phone_number}|${sms.text}`;
    if (!groups[key]) groups[key] = [];
    groups[key].push(sms);
  });
  
  // Find duplicates (keep most recent)
  const duplicatesToDelete = [];
  for (const [key, messages] of Object.entries(groups)) {
    if (messages.length > 1) {
      // Sort by created_at descending, keep first, delete rest
      messages.sort((a, b) => new Date(b.created_at) - new Date(a.created_at));
      duplicatesToDelete.push(...messages.slice(1));
    }
  }
  
  if (duplicatesToDelete.length === 0) {
    console.log('  ‚ÑπÔ∏è  No duplicates found\n');
    return { deleted: 0 };
  }
  
  console.log(`  Found ${duplicatesToDelete.length} duplicate messages to delete`);
  
  // Delete duplicates
  const idsToDelete = duplicatesToDelete.map(s => s.id);
  let deletedCount = 0;
  
  for (let i = 0; i < idsToDelete.length; i += 50) {
    const batch = idsToDelete.slice(i, i + 50);
    const { error: deleteError } = await supabase
      .from('sms')
      .delete()
      .in('id', batch);
    
    if (deleteError) {
      console.log(`  ‚ö†Ô∏è  Error deleting batch: ${deleteError.message}`);
    } else {
      deletedCount += batch.length;
      process.stdout.write(`    Progress: ${deletedCount}/${duplicatesToDelete.length}\r`);
    }
  }
  
  console.log(`\n  ‚úÖ Deleted ${deletedCount} duplicate SMS messages\n`);
  
  return { deleted: deletedCount };
}

async function finalVerification() {
  console.log('\nüì¶ STEP 6: Final verification...\n');
  
  // Check counts
  console.log('  Checking table counts:');
  
  const tables = [
    'cashback_codes',
    'cashback_codes_archive', 
    'tiler_transaction',
    'tiler_transaction_archive',
    'sms'
  ];
  
  for (const table of tables) {
    const { data, error } = await supabase
      .from(table)
      .select('*', { count: 'exact', head: true });
    
    if (error) {
      console.log(`    ‚ö†Ô∏è  ${table}: Error - ${error.message}`);
    } else {
      console.log(`    ${table}: ${data.length || 0} records`);
    }
  }
  
  console.log('\n  ‚úÖ Cleanup complete!');
  console.log('  üìä Summary:');
  console.log('     - Old redeemed codes moved to archive');
  console.log('     - Old transactions moved to archive');
  console.log('     - Duplicate SMS removed');
  console.log('     - All data preserved in backup files');
  console.log('\n  üéâ Database should now be under 0.5 GB limit!\n');
}

async function runCleanup() {
  console.log('‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó');
  console.log('‚ïë         SAFE DATABASE CLEANUP - EXECUTION             ‚ïë');
  console.log('‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n');
  
  console.log('‚ö†Ô∏è  This will:');
  console.log('   1. Export all critical data (safety backup)');
  console.log('   2. Archive 1000 old redeemed cashback codes');
  console.log('   3. Archive 92 old tiler transactions');
  console.log('   4. Delete 31 duplicate SMS messages');
  console.log('   5. Verify data integrity\n');
  
  console.log('‚úÖ Safety measures:');
  console.log('   - All data exported before changes');
  console.log('   - Archived data preserved in tables');
  console.log('   - Rollback possible via exported files');
  console.log('   - Verification after each step\n');
  
  // Execute each step
  for (let i = 0; i < STEPS.length; i++) {
    const step = STEPS[i];
    console.log(`\n${'='.repeat(60)}`);
    console.log(`Step ${i + 1}/${STEPS.length}: ${step.name}`);
    console.log(`${'='.repeat(60)}`);
    
    try {
      await step.action();
    } catch (error) {
      console.error(`\n‚ùå Step ${i + 1} failed: ${error.message}`);
      console.log('\nüõë Stopping cleanup. Data is safe in exported files.');
      console.log('   You can restore from: backups/pre-cleanup-export-[date]/\n');
      process.exit(1);
    }
  }
  
  console.log('\n' + '='.repeat(60));
  console.log('‚úÖ ALL STEPS COMPLETED SUCCESSFULLY!');
  console.log('='.repeat(60) + '\n');
  console.log('üìã Next steps:');
  console.log('   1. Check Supabase Dashboard for new database size');
  console.log('   2. Test application functionality');
  console.log('   3. If needed, run VACUUM in Supabase SQL Editor:');
  console.log('      VACUUM FULL;');
  console.log('\nüìÅ Backup location:');
  console.log('   backups/pre-cleanup-export-[today]/\n');
}

runCleanup().catch(error => {
  console.error('\n‚ùå Fatal error:', error.message);
  process.exit(1);
});
